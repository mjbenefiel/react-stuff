Algorithms and Data Structures | Big O Notation

- Intro to Big O Notation
    - Way of formalized fuzzy counting
    - Allows us to talk formally about how the runtime of an algorithm grows as input grows
    - Focus on trends, not minute details

    - We say that an algorithm is O(f(n)) if the number of simple operations the computer has to do is eventually less than a constant f(n), as n increases
        - f(n) could be linear (f(n) = n)
        - f(n) could be quadratic (f(n) = n2) **n squared
        - f(n) could be constant (f(n) = 1)
        - f(n) could be something entirely different

- Problem with Timers
    - Different machines will record different times
    - Same machine will record different times

- Counting Operations

